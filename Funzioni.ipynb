{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMGkc+u2oPVJSFpGUtW2/D6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccsed/ProgettoLabAI/blob/main/Funzioni.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collego google drive"
      ],
      "metadata": {
        "id": "eW1UMwzUbOzX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhOZCbCwbGsz",
        "outputId": "ad07b09b-27b7-4715-f449-dc237bf173b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importo librerie"
      ],
      "metadata": {
        "id": "6aubrgChbqTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6YyKXyGbpQg",
        "outputId": "82dff318-7ac4-4e65-f6a9-6239309a2b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.10-cp310-cp310-manylinux2014_x86_64.whl (21.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2024.2.2)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.25.2)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.2)\n",
            "Installing collected packages: snuggs, affine, rasterio\n",
            "Successfully installed affine-2.4.0 rasterio-1.3.10 snuggs-1.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "import albumentations as A\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import skimage.io as io\n",
        "from rasterio.plot import show\n",
        "import cv2"
      ],
      "metadata": {
        "id": "ksutkuvTb0h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU"
      ],
      "metadata": {
        "id": "GRGZz-5UcJqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "7VnvB5c8cIlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funzione per dividere il dataset in training, validation e test set"
      ],
      "metadata": {
        "id": "pL31LDNpb6XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(image_dir, mask_dir, drive_dir, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, seed=42):\n",
        "    images = sorted(os.listdir(image_dir))\n",
        "    masks = sorted(os.listdir(mask_dir))\n",
        "\n",
        "    print(len(os.listdir(image_dir)))\n",
        "    print(len(os.listdir(mask_dir)))\n",
        "\n",
        "    # Controllo se il numero di immagini e maschere è lo stesso\n",
        "    assert len(images) == len(masks), \"Il numero di immagini e maschere non corrisponde\"\n",
        "\n",
        "    combined = list(zip(images, masks))\n",
        "    random.seed(seed)\n",
        "    random.shuffle(combined)\n",
        "\n",
        "    train_split = int(len(combined) * train_ratio)\n",
        "    val_split = int(len(combined) * (train_ratio + val_ratio))\n",
        "    train_data = combined[:train_split]\n",
        "    val_data = combined[train_split:val_split]\n",
        "    test_data = combined[val_split:]\n",
        "\n",
        "    # Scrivere i percorsi dei file nelle liste train, val e test\n",
        "    train_files = [(os.path.join(image_dir, img), os.path.join(mask_dir, lbl)) for img, lbl in train_data]\n",
        "    val_files = [(os.path.join(image_dir, img), os.path.join(mask_dir, lbl)) for img, lbl in val_data]\n",
        "    test_files = [(os.path.join(image_dir, img), os.path.join(mask_dir, lbl)) for img, lbl in test_data]\n",
        "\n",
        "    # Scrivere i percorsi dei file nei file .txt\n",
        "    with open(os.path.join(drive_dir, 'train.txt'), 'w') as f:\n",
        "        for img_path, lbl_path in train_files:\n",
        "            f.write(f\"{img_path} {lbl_path}\\n\")\n",
        "\n",
        "    with open(os.path.join(drive_dir, 'val.txt'), 'w') as f:\n",
        "        for img_path, lbl_path in val_files:\n",
        "            f.write(f\"{img_path} {lbl_path}\\n\")\n",
        "\n",
        "    with open(os.path.join(drive_dir, 'test.txt'), 'w') as f:\n",
        "        for img_path, lbl_path in test_files:\n",
        "            f.write(f\"{img_path} {lbl_path}\\n\")\n"
      ],
      "metadata": {
        "id": "wwtfTbGpb58w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "def split_dataset(image_dir, mask_dir, drive_dir, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, seed=42):\n",
        "    images = sorted(os.listdir(image_dir))\n",
        "    masks = sorted(os.listdir(mask_dir))\n",
        "\n",
        "    print(f\"Numero di immagini: {len(images)}\")\n",
        "    print(f\"Numero di maschere: {len(masks)}\")\n",
        "\n",
        "\n",
        "    assert len(images) == len(masks), \"Il numero di immagini e maschere non corrisponde\"\n",
        "\n",
        "    combined = list(zip(images, masks))\n",
        "    random.seed(seed)\n",
        "    random.shuffle(combined)\n",
        "\n",
        "    train_split = int(len(combined) * train_ratio)\n",
        "    val_split = int(len(combined) * (train_ratio + val_ratio))\n",
        "    train_data = combined[:train_split]\n",
        "    val_data = combined[train_split:val_split]\n",
        "    test_data = combined[val_split:]\n",
        "\n",
        "\n",
        "    for folder in ['train_images', 'train_masks', 'val_images', 'val_masks', 'test_images', 'test_masks']:\n",
        "        os.makedirs(os.path.join(drive_dir, folder), exist_ok=True)\n",
        "\n",
        "    def move_files(file_list, image_dest, mask_dest, description):\n",
        "        for img, lbl in tqdm(file_list, desc=description):\n",
        "            shutil.move(os.path.join(image_dir, img), os.path.join(image_dest, img))\n",
        "            shutil.move(os.path.join(mask_dir, lbl), os.path.join(mask_dest, lbl))\n",
        "\n",
        "\n",
        "    move_files(train_data, os.path.join(drive_dir, 'train_images'), os.path.join(drive_dir, 'train_masks'), 'Spostamento Train')\n",
        "    move_files(val_data, os.path.join(drive_dir, 'val_images'), os.path.join(drive_dir, 'val_masks'), 'Spostamento Val')\n",
        "    move_files(test_data, os.path.join(drive_dir, 'test_images'), os.path.join(drive_dir, 'test_masks'), 'Spostamento Test')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4lv50Iwf2Obg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco alcune directory"
      ],
      "metadata": {
        "id": "sPRh2tZucRL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_dir = '/content/drive/MyDrive'\n",
        "image_dir = '/content/drive/MyDrive/Data/SN6_buildings_AOI_11_Rotterdam_train/train/AOI_11_Rotterdam/SAR-Intensity'\n",
        "train_path = '/content/drive/MyDrive/train.txt'\n",
        "val_path = '/content/drive/MyDrive/val.txt'\n",
        "test_path = '/content/drive/MyDrive/test.txt'\n",
        "label_dir = drive_dir + '/Labels'"
      ],
      "metadata": {
        "id": "FF9651qpcUKm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faccio split del dataset"
      ],
      "metadata": {
        "id": "qA3FPL4Rcbtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset(image_dir, label_dir, drive_dir)"
      ],
      "metadata": {
        "id": "lCXnKL9fcfEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee7733b-61a6-46c9-8a46-d470a675297b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero di immagini: 3401\n",
            "Numero di maschere: 3401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spostamento Train: 100%|██████████| 2380/2380 [00:12<00:00, 190.92it/s]\n",
            "Spostamento Val: 100%|██████████| 680/680 [00:03<00:00, 201.58it/s]\n",
            "Spostamento Test: 100%|██████████| 341/341 [00:01<00:00, 190.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "def restore_dataset(original_image_dir, original_mask_dir, split_dir):\n",
        "    # Directory di origine dei file suddivisi\n",
        "    split_folders = {\n",
        "        'train_images': original_image_dir,\n",
        "        'train_masks': original_mask_dir,\n",
        "        'val_images': original_image_dir,\n",
        "        'val_masks': original_mask_dir,\n",
        "        'test_images': original_image_dir,\n",
        "        'test_masks': original_mask_dir\n",
        "    }\n",
        "\n",
        "    # Funzione per spostare i file indietro con barra di avanzamento\n",
        "    def move_files_back(src_dir, dest_dir, description):\n",
        "        files = os.listdir(src_dir)\n",
        "        for file in tqdm(files, desc=description):\n",
        "            shutil.move(os.path.join(src_dir, file), os.path.join(dest_dir, file))\n",
        "\n",
        "    # Spostare i file indietro\n",
        "    for folder, dest in split_folders.items():\n",
        "        src_dir = os.path.join(split_dir, folder)\n",
        "        move_files_back(src_dir, dest, f\"Spostamento {folder}\")\n",
        "\n",
        "\n",
        "restore_dataset(image_dir, label_dir, drive_dir)\n"
      ],
      "metadata": {
        "id": "0IkCG_lR5xq0",
        "outputId": "7b4e68a5-da27-4390-edcb-2a7070a26213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spostamento train_images: 0it [00:00, ?it/s]\n",
            "Spostamento train_masks: 0it [00:00, ?it/s]\n",
            "Spostamento val_images: 0it [00:00, ?it/s]\n",
            "Spostamento val_masks: 0it [00:00, ?it/s]\n",
            "Spostamento test_images: 0it [00:00, ?it/s]\n",
            "Spostamento test_masks: 0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path delle immagini"
      ],
      "metadata": {
        "id": "3oh4YvHGnDjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(train_path, 'r') as f:\n",
        "    train_data = [line.split() for line in f.read().splitlines()]\n",
        "\n",
        "train_image_paths = [line[0] for line in train_data]\n",
        "\n",
        "with open(val_path, 'r') as f:\n",
        "    val_data = [line.split() for line in f.read().splitlines()]\n",
        "\n",
        "val_image_paths = [line[0] for line in val_data]\n",
        "\n",
        "with open(test_path, 'r') as f:\n",
        "    test_data = [line.split() for line in f.read().splitlines()]\n",
        "\n",
        "test_image_paths = [line[0] for line in test_data]"
      ],
      "metadata": {
        "id": "umnzcJBGnHd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funzione per calcolare media e deviazione standard sul training set"
      ],
      "metadata": {
        "id": "Gfekyixylakc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mean_std(image_paths):\n",
        "    mean = np.zeros(4)\n",
        "    std = np.zeros(4)\n",
        "    nb_samples = 0\n",
        "\n",
        "    # Abilita l'uso della GPU per OpenCV\n",
        "    cv2.setUseOptimized(True)\n",
        "    cv2.setNumThreads(8)\n",
        "    cv2.ocl.setUseOpenCL(True)\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        try:\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "            for i in range(4):\n",
        "                mean[i] += np.mean(image[:,:,i])\n",
        "                std[i] += np.std(image[:,:,i])\n",
        "\n",
        "            nb_samples += 1\n",
        "\n",
        "            if nb_samples % 10 == 0:\n",
        "                print(f\"Processed {nb_samples} images\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image: {image_path}\")\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "HT2E5pkHlgfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcolo media e std su training set"
      ],
      "metadata": {
        "id": "2Cab5lfynuL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = compute_mean_std(train_image_paths)\n",
        "print(\"Mean: \", mean)\n",
        "print(\"Std: \", std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjxuHeNnnyjq",
        "outputId": "59d7b238-1dbf-4134-c9e8-5c0f10d23337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10 images\n",
            "Processed 20 images\n",
            "Processed 30 images\n",
            "Processed 40 images\n",
            "Processed 50 images\n",
            "Processed 60 images\n",
            "Processed 70 images\n",
            "Processed 80 images\n",
            "Processed 90 images\n",
            "Processed 100 images\n",
            "Processed 110 images\n",
            "Processed 120 images\n",
            "Processed 130 images\n",
            "Processed 140 images\n",
            "Processed 150 images\n",
            "Processed 160 images\n",
            "Processed 170 images\n",
            "Processed 180 images\n",
            "Processed 190 images\n",
            "Processed 200 images\n",
            "Processed 210 images\n",
            "Processed 220 images\n",
            "Processed 230 images\n",
            "Processed 240 images\n",
            "Processed 250 images\n",
            "Processed 260 images\n",
            "Processed 270 images\n",
            "Processed 280 images\n",
            "Processed 290 images\n",
            "Processed 300 images\n",
            "Processed 310 images\n",
            "Processed 320 images\n",
            "Processed 330 images\n",
            "Processed 340 images\n",
            "Processed 350 images\n",
            "Processed 360 images\n",
            "Processed 370 images\n",
            "Processed 380 images\n",
            "Processed 390 images\n",
            "Processed 400 images\n",
            "Processed 410 images\n",
            "Processed 420 images\n",
            "Processed 430 images\n",
            "Processed 440 images\n",
            "Processed 450 images\n",
            "Processed 460 images\n",
            "Processed 470 images\n",
            "Processed 480 images\n",
            "Processed 490 images\n",
            "Processed 500 images\n",
            "Processed 510 images\n",
            "Processed 520 images\n",
            "Processed 530 images\n",
            "Processed 540 images\n",
            "Processed 550 images\n",
            "Processed 560 images\n",
            "Processed 570 images\n",
            "Processed 580 images\n",
            "Processed 590 images\n",
            "Processed 600 images\n",
            "Processed 610 images\n",
            "Processed 620 images\n",
            "Processed 630 images\n",
            "Processed 640 images\n",
            "Processed 650 images\n",
            "Processed 660 images\n",
            "Processed 670 images\n",
            "Processed 680 images\n",
            "Processed 690 images\n",
            "Processed 700 images\n",
            "Processed 710 images\n",
            "Processed 720 images\n",
            "Processed 730 images\n",
            "Processed 740 images\n",
            "Processed 750 images\n",
            "Processed 760 images\n",
            "Processed 770 images\n",
            "Processed 780 images\n",
            "Processed 790 images\n",
            "Processed 800 images\n",
            "Processed 810 images\n",
            "Processed 820 images\n",
            "Processed 830 images\n",
            "Processed 840 images\n",
            "Processed 850 images\n",
            "Processed 860 images\n",
            "Processed 870 images\n",
            "Processed 880 images\n",
            "Processed 890 images\n",
            "Processed 900 images\n",
            "Processed 910 images\n",
            "Processed 920 images\n",
            "Processed 930 images\n",
            "Processed 940 images\n",
            "Processed 950 images\n",
            "Processed 960 images\n",
            "Processed 970 images\n",
            "Processed 980 images\n",
            "Processed 990 images\n",
            "Processed 1000 images\n",
            "Processed 1010 images\n",
            "Processed 1020 images\n",
            "Processed 1030 images\n",
            "Processed 1040 images\n",
            "Processed 1050 images\n",
            "Processed 1060 images\n",
            "Processed 1070 images\n",
            "Processed 1080 images\n",
            "Processed 1090 images\n",
            "Processed 1100 images\n",
            "Processed 1110 images\n",
            "Processed 1120 images\n",
            "Processed 1130 images\n",
            "Processed 1140 images\n",
            "Processed 1150 images\n",
            "Processed 1160 images\n",
            "Processed 1170 images\n",
            "Processed 1180 images\n",
            "Processed 1190 images\n",
            "Processed 1200 images\n",
            "Processed 1210 images\n",
            "Processed 1220 images\n",
            "Processed 1230 images\n",
            "Processed 1240 images\n",
            "Processed 1250 images\n",
            "Processed 1260 images\n",
            "Processed 1270 images\n",
            "Processed 1280 images\n",
            "Processed 1290 images\n",
            "Processed 1300 images\n",
            "Processed 1310 images\n",
            "Processed 1320 images\n",
            "Processed 1330 images\n",
            "Processed 1340 images\n",
            "Processed 1350 images\n",
            "Processed 1360 images\n",
            "Processed 1370 images\n",
            "Processed 1380 images\n",
            "Processed 1390 images\n",
            "Processed 1400 images\n",
            "Processed 1410 images\n",
            "Processed 1420 images\n",
            "Processed 1430 images\n",
            "Processed 1440 images\n",
            "Processed 1450 images\n",
            "Processed 1460 images\n",
            "Processed 1470 images\n",
            "Processed 1480 images\n",
            "Processed 1490 images\n",
            "Processed 1500 images\n",
            "Processed 1510 images\n",
            "Processed 1520 images\n",
            "Processed 1530 images\n",
            "Processed 1540 images\n",
            "Processed 1550 images\n",
            "Processed 1560 images\n",
            "Processed 1570 images\n",
            "Processed 1580 images\n",
            "Processed 1590 images\n",
            "Processed 1600 images\n",
            "Processed 1610 images\n",
            "Processed 1620 images\n",
            "Processed 1630 images\n",
            "Processed 1640 images\n",
            "Processed 1650 images\n",
            "Processed 1660 images\n",
            "Processed 1670 images\n",
            "Processed 1680 images\n",
            "Processed 1690 images\n",
            "Processed 1700 images\n",
            "Processed 1710 images\n",
            "Processed 1720 images\n",
            "Processed 1730 images\n",
            "Processed 1740 images\n",
            "Processed 1750 images\n",
            "Processed 1760 images\n",
            "Processed 1770 images\n",
            "Processed 1780 images\n",
            "Processed 1790 images\n",
            "Processed 1800 images\n",
            "Processed 1810 images\n",
            "Processed 1820 images\n",
            "Processed 1830 images\n",
            "Processed 1840 images\n",
            "Processed 1850 images\n",
            "Processed 1860 images\n",
            "Processed 1870 images\n",
            "Processed 1880 images\n",
            "Processed 1890 images\n",
            "Processed 1900 images\n",
            "Processed 1910 images\n",
            "Processed 1920 images\n",
            "Processed 1930 images\n",
            "Processed 1940 images\n",
            "Processed 1950 images\n",
            "Processed 1960 images\n",
            "Processed 1970 images\n",
            "Processed 1980 images\n",
            "Processed 1990 images\n",
            "Processed 2000 images\n",
            "Processed 2010 images\n",
            "Processed 2020 images\n",
            "Processed 2030 images\n",
            "Processed 2040 images\n",
            "Processed 2050 images\n",
            "Processed 2060 images\n",
            "Processed 2070 images\n",
            "Processed 2080 images\n",
            "Processed 2090 images\n",
            "Processed 2100 images\n",
            "Processed 2110 images\n",
            "Processed 2120 images\n",
            "Processed 2130 images\n",
            "Processed 2140 images\n",
            "Processed 2150 images\n",
            "Processed 2160 images\n",
            "Processed 2170 images\n",
            "Processed 2180 images\n",
            "Processed 2190 images\n",
            "Processed 2200 images\n",
            "Processed 2210 images\n",
            "Processed 2220 images\n",
            "Processed 2230 images\n",
            "Processed 2240 images\n",
            "Processed 2250 images\n",
            "Processed 2260 images\n",
            "Processed 2270 images\n",
            "Processed 2280 images\n",
            "Processed 2290 images\n",
            "Processed 2300 images\n",
            "Processed 2310 images\n",
            "Processed 2320 images\n",
            "Processed 2330 images\n",
            "Processed 2340 images\n",
            "Processed 2350 images\n",
            "Processed 2360 images\n",
            "Processed 2370 images\n",
            "Processed 2380 images\n",
            "Mean:  [22.79312241 24.30391245 19.29892107 17.69439361]\n",
            "Std:  [16.42519281 17.42876255 14.27711349 13.24714869]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funzioni per convertire i file geojson in tiff"
      ],
      "metadata": {
        "id": "AsfvcaOWObYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_mask_from_labels(geojson_path, reference_image_path, output_tiff_path):\n",
        "    try:\n",
        "        with rasterio.open(reference_image_path) as src:\n",
        "            transform = src.transform\n",
        "            out_shape = (src.height, src.width)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error reading reference TIFF file {reference_image_path}: {e}\")\n",
        "\n",
        "    try:\n",
        "        gdf = gpd.read_file(geojson_path)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error reading GeoJSON file {geojson_path}: {e}\")\n",
        "\n",
        "    if gdf.empty or gdf.geometry.isnull().all():\n",
        "        mask = np.zeros(out_shape, dtype='uint8')\n",
        "    else:\n",
        "        mask = rasterize(\n",
        "            [(geom, 255) for geom in gdf.geometry if geom.is_valid],\n",
        "            out_shape=out_shape,\n",
        "            transform=transform,\n",
        "            fill=0,\n",
        "            dtype='uint8'\n",
        "        )\n",
        "\n",
        "    with rasterio.open(\n",
        "        output_tiff_path, 'w',\n",
        "        driver='GTiff',\n",
        "        height=mask.shape[0],\n",
        "        width=mask.shape[1],\n",
        "        count=1,\n",
        "        dtype=mask.dtype,\n",
        "        crs='+proj=latlong',\n",
        "        transform=transform,\n",
        "    ) as dst:\n",
        "        dst.write(mask, 1)\n",
        "\n",
        "\n",
        "def process_dataset(geojson_directory, reference_image_directory, output_tiff_directory):\n",
        "\n",
        "    if not os.path.exists(output_tiff_directory):\n",
        "        os.makedirs(output_tiff_directory)\n",
        "    geojson_files = [f for f in os.listdir(geojson_directory) if f.endswith('.geojson')]\n",
        "    count = 0\n",
        "    for geojson_file in geojson_files:\n",
        "        count += 1\n",
        "        if count%20 == 0:\n",
        "           print(count)\n",
        "        geojson_path = os.path.join(geojson_directory, geojson_file)\n",
        "        reference_image_path = os.path.join(reference_image_directory, geojson_file.replace('.geojson', '.tif').replace('Buildings', 'PS-RGB'))\n",
        "        output_tiff_path = os.path.join(output_tiff_directory, geojson_file.replace('.geojson', '_mask.tiff'))\n",
        "\n",
        "        if os.path.exists(output_tiff_path):\n",
        "            print(f\"{output_tiff_path} already exists, skipping...\")\n",
        "            continue\n",
        "\n",
        "        if not os.path.exists(reference_image_path):\n",
        "            print(f\"Reference image not found for {geojson_file}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            create_mask_from_labels(geojson_path, reference_image_path, output_tiff_path)\n",
        "            print(f\"Converted {geojson_file} to {output_tiff_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to convert {geojson_file}: {e}\")\n"
      ],
      "metadata": {
        "id": "IZgAO3xKvelV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geojson_dir = '/content/drive/MyDrive/Data/SN6_buildings_AOI_11_Rotterdam_train/train/AOI_11_Rotterdam/geojson_buildings'\n",
        "rgb_dir = '/content/drive/MyDrive/Data/SN6_buildings_AOI_11_Rotterdam_train/train/AOI_11_Rotterdam/PS-RGB'\n",
        "output_dir = '/content/drive/MyDrive/Labels'\n",
        "\n",
        "process_dataset(geojson_dir, rgb_dir, output_dir)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-X5SsQ89CGE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}